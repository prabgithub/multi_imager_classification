{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c14502b-d2f7-49ce-b705-b2db92bac706",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step-3 Notebook for combining the Flowcam and IFCB datasets and storing the training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c25b10e-9c16-4a44-951f-08bb5b7e9da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21727992-767f-479a-967b-630f1d88b9de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import sys\n",
    "from os import listdir, path\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062017a7-a059-440c-8669-fc8f4abd6a7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load the stage 1 processed data for each of flowcam and ifcb datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d636db5b-e7af-403c-a0d2-926996e03b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fc_data = os.listdir(\"./flowcam_processed_1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad73c2bf-711d-4b4d-b3b6-4703cb9b70c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ifcb_data = os.listdir(\"./ifcb_processed_1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6605fe-c27d-491d-bfd4-7ef369d61a1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare a common category list between two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2544f66-f210-4d79-8d0c-fd670eae1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_list=[]\n",
    "for i in fc_data:\n",
    "    if i == \".ipynb_checkpoints\":\n",
    "        continue\n",
    "    if i in ifcb_data:\n",
    "        common_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5dd7eec-33ca-4d20-8384-318463a12ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_list.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337f3078-c154-4b1e-adae-3695d9072cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "j=0\n",
    "common_cats_path_ifcb=[]\n",
    "for i in glob.glob(\"./ifcb_processed_1/*\"):\n",
    "    idx = os.path.basename(i)\n",
    "    if idx in common_list:\n",
    "        j+=1\n",
    "        print(j,'----',i)\n",
    "        common_cats_path_ifcb.append(i)\n",
    "        \n",
    "j=0\n",
    "common_cats_path_fc=[]\n",
    "for i in glob.glob(\"./flowcam_processed_1/*\"):\n",
    "    idx = os.path.basename(i)\n",
    "    if idx in common_list:\n",
    "        j+=1\n",
    "        print(j,'----',i)\n",
    "        common_cats_path_fc.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a8a412-6c1a-444c-8dd8-ec68641bda6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_cats_path_fc.__len__(),common_cats_path_ifcb.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569ce418-2e10-4edd-ae5a-3bacdcbbf771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "for i in common_cats_path_fc:\n",
    "    k+=1\n",
    "    for j in glob.glob(f'{i}/*'):\n",
    "        if j.split('.')[-1] == 'npy':\n",
    "            print(k,'--',os.path.basename(i),'--',os.path.basename(j))\n",
    "            data = np.load(j,allow_pickle=True)\n",
    "            print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc492186-5479-4d01-8254-59be707d5e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "for i in common_cats_path_ifcb:\n",
    "    k+=1\n",
    "    for j in glob.glob(f'{i}/*'):\n",
    "        if j.split('.')[-1] == 'npy':\n",
    "            print(k,'--',os.path.basename(i),'--',os.path.basename(j))\n",
    "            data = np.load(j,allow_pickle=True)\n",
    "            print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb926b42-1edc-4773-a53b-f7094fb5733f",
   "metadata": {},
   "source": [
    "### Add and merge the dataset from Flowcam data to combined processed stage-1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7035ebb0-f211-445f-89e7-797517b5ce79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 passed creating ./common_processed_data/Euterpina_copepod\n",
      "unique saved at-->  ./common_processed_data/Euterpina_copepod/Euterpina_copepod_im.npy\n",
      "2 passed creating ./common_processed_data/Empty_lorica\n",
      "unique saved at-->  ./common_processed_data/Empty_lorica/Empty_lorica_im.npy\n",
      "3 passed creating ./common_processed_data/Echinoderm_larvae\n",
      "unique saved at-->  ./common_processed_data/Echinoderm_larvae/Echinoderm_larvae_im.npy\n",
      "4 passed creating ./common_processed_data/Eutintinnus_tintinnid\n",
      "unique saved at-->  ./common_processed_data/Eutintinnus_tintinnid/Eutintinnus_tintinnid_im.npy\n",
      "not_included--> 0 common--> 0 unique--> 4\n"
     ]
    }
   ],
   "source": [
    "fc_path = glob.glob(\"./flowcam_processed_1/*\")\n",
    "common_dir=\"./common_processed_data\"\n",
    "os.makedirs(common_dir, exist_ok= True)\n",
    "k=0\n",
    "not_included=0\n",
    "common=0\n",
    "unique=0\n",
    "for i in fc_path:\n",
    "    k+=1\n",
    "    for j in glob.glob(f'{i}/*'):\n",
    "        if j.split('.')[-1] == 'npy':\n",
    "            data = np.load(j,allow_pickle=True)\n",
    "            if data.shape[0] == 1:\n",
    "                print(f\"{k} not included\",'--',os.path.basename(i),'--',os.path.basename(j))\n",
    "                print(data.shape)\n",
    "                not_included+=1\n",
    "            elif i in common_cats_path_fc:\n",
    "                print(f\"{k} belongs to common cat\",'--',os.path.basename(i),'--',os.path.basename(j))\n",
    "                for l in common_cats_path_ifcb:\n",
    "                    if os.path.basename(l) == os.path.basename(i):\n",
    "                        common+=1\n",
    "                        dir_path= os.path.join(common_dir,os.path.basename(i))\n",
    "                        print(os.path.basename(l),os.path.basename(i))\n",
    "                        data1 = np.load(j,allow_pickle=True)\n",
    "                        for m in glob.glob(f'{l}/*'):\n",
    "                            if m.split('.')[-1] == 'npy':\n",
    "                                data2 = np.load(m,allow_pickle=True)\n",
    "                        data3=np.empty(data1.shape[0]+data2.shape[0],dtype=object)\n",
    "                        data3[:data1.shape[0]]=data1\n",
    "                        data3[data1.shape[0]:]=data2\n",
    "                        print(data1.shape,data2.shape,data3.shape,data3[:data1.shape[0]][-1].shape,data3[data1.shape[0]:][-1].shape)\n",
    "                        print(data1[-1].shape,data2[-1].shape)\n",
    "                        os.makedirs(dir_path,exist_ok=True)\n",
    "                        path_save = os.path.join(dir_path,os.path.basename(j))\n",
    "                        np.save(path_save,data3)\n",
    "                        print(\"unique saved at--> \",path_save)\n",
    "            else:\n",
    "                dir_path= os.path.join(common_dir,os.path.basename(i))\n",
    "                print(f\"{k} passed creating {dir_path}\")\n",
    "                os.makedirs(dir_path,exist_ok=True)\n",
    "                path_save = os.path.join(dir_path,os.path.basename(j))\n",
    "                np.save(path_save,data)\n",
    "                print(\"unique saved at--> \",path_save)\n",
    "                unique+=1\n",
    "print('not_included-->',not_included,'common-->',common,'unique-->',unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b351bc92-44df-4d46-90da-94dd9d2c48ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"./common_processed_data\").__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c1678f-1ed6-4a5c-b66f-f02efac8bfbf",
   "metadata": {},
   "source": [
    "### Add and merge the datasets from IFCB data to combined processed stage-1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b68215-bc8d-4219-9cea-0d6b2e388155",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 passed creatiing ./common_processed_data/Acantharian\n",
      "2 passed creatiing ./common_processed_data/Alexandrium\n",
      "3 passed creatiing ./common_processed_data/Acanthoica_quattrospina\n",
      "4 passed creatiing ./common_processed_data/Amphorelloides_tropidoneis\n",
      "5 passed creatiing ./common_processed_data/Amphidinium_crassipes\n",
      "6 passed creatiing ./common_processed_data/Askenasia\n",
      "7 passed creatiing ./common_processed_data/Asterompalus_flabellatus\n",
      "not_included--> 0 common--> 0 unique--> 7\n"
     ]
    }
   ],
   "source": [
    "fc_path = glob.glob(\"./flowcam_processed_1/*\")\n",
    "ifcb_path = glob.glob(\"./ifcb_processed_1/*\")\n",
    "common_dir=\"./common_processed_data\"\n",
    "k=0\n",
    "not_included=0\n",
    "common=0\n",
    "unique=0\n",
    "for i in ifcb_path:\n",
    "    k+=1\n",
    "    for j in glob.glob(f'{i}/*'):\n",
    "        if j.split('.')[-1] == 'npy':\n",
    "            data = np.load(j,allow_pickle=True)\n",
    "            if data.shape[0] == 1:\n",
    "                print(f\"{k} not included\",'--',os.path.basename(i),'--',os.path.basename(j))\n",
    "                print(data.shape)\n",
    "                not_included+=1\n",
    "            elif i in common_cats_path_ifcb:\n",
    "                print(f\"{k} belongs to common cat\",'--',os.path.basename(i),'--',os.path.basename(j))\n",
    "                common+=1\n",
    "            else:\n",
    "                dir_path= os.path.join(common_dir,os.path.basename(i))\n",
    "                print(f\"{k} passed creatiing {dir_path}\")\n",
    "                os.makedirs(dir_path,exist_ok=True)\n",
    "                path_save=os.path.join(dir_path,os.path.basename(j))\n",
    "                np.save(path_save,data)\n",
    "                unique+=1\n",
    "print('not_included-->',not_included,'common-->',common,'unique-->',unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c7658d2-d82f-4ddd-9ee9-cb4b5d38f388",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(os.listdir(\"./common_processed_data\")).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64ff21ab-2f4c-4605-8914-1e0d03007c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- 0.0 ---- (2, 1) (2,)\n",
      "1 ---- 1.0 ---- (21, 1) (21,)\n",
      "2 ---- 2.0 ---- (59, 1) (59,)\n",
      "3 ---- 3.0 ---- (44, 1) (44,)\n",
      "4 ---- 4.0 ---- (46, 1) (46,)\n",
      "5 ---- 5.0 ---- (62, 1) (62,)\n",
      "6 ---- 6.0 ---- (4, 1) (4,)\n",
      "7 ---- 7.0 ---- (16, 1) (16,)\n",
      "8 ---- 8.0 ---- (58, 1) (58,)\n",
      "9 ---- 9.0 ---- (10, 1) (10,)\n",
      "10 ---- 10.0 ---- (2, 1) (2,)\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for m,i in enumerate(glob.glob(\"./common_processed_data/*\")):\n",
    "    for j in glob.glob(f\"{i}/*.npy\"):\n",
    "        data=np.load(j, allow_pickle=True)\n",
    "        n=len(data)\n",
    "        df=pd.DataFrame(columns=['_target'])\n",
    "        df._target=np.ones(n)*m\n",
    "        df.to_csv(os.path.join(i,f\"{os.path.basename(i)}_pd.csv\"))\n",
    "        print(m,'----',df._target[0],'----',df.shape,data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab9efd-6deb-4faa-9f4f-abb0e5ceb54f",
   "metadata": {},
   "source": [
    "### Process and store the stage-1 dataset to stage-2 merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a94a788-0545-4ab1-ae86-679aec861e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ea26610-4837-4bb1-86f9-2bd1ede9457f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "2\n",
      "unique--> [0]\n",
      "resized_shape--> (2, 101, 64, 3)\n",
      "******************** Acantharian idx--> 0 ********************\n",
      "(21, 2)\n",
      "21\n",
      "unique--> [1]\n",
      "resized_shape--> (21, 101, 64, 3)\n",
      "******************** Alexandrium idx--> 1 ********************\n",
      "(59, 2)\n",
      "59\n",
      "unique--> [2]\n",
      "resized_shape--> (59, 101, 64, 3)\n",
      "******************** Acanthoica_quattrospina idx--> 2 ********************\n",
      "(44, 2)\n",
      "44\n",
      "unique--> [3]\n",
      "resized_shape--> (44, 101, 64, 3)\n",
      "******************** Euterpina_copepod idx--> 3 ********************\n",
      "(46, 2)\n",
      "46\n",
      "unique--> [4]\n",
      "resized_shape--> (46, 101, 64, 3)\n",
      "******************** Empty_lorica idx--> 4 ********************\n",
      "(62, 2)\n",
      "62\n",
      "unique--> [5]\n",
      "resized_shape--> (62, 101, 64, 3)\n",
      "******************** Amphorelloides_tropidoneis idx--> 5 ********************\n",
      "(4, 2)\n",
      "4\n",
      "unique--> [6]\n",
      "resized_shape--> (4, 101, 64, 3)\n",
      "******************** Amphidinium_crassipes idx--> 6 ********************\n",
      "(16, 2)\n",
      "16\n",
      "unique--> [7]\n",
      "resized_shape--> (16, 101, 64, 3)\n",
      "******************** Echinoderm_larvae idx--> 7 ********************\n",
      "(58, 2)\n",
      "58\n",
      "unique--> [8]\n",
      "resized_shape--> (58, 101, 64, 3)\n",
      "******************** Eutintinnus_tintinnid idx--> 8 ********************\n",
      "(10, 2)\n",
      "10\n",
      "unique--> [9]\n",
      "resized_shape--> (10, 101, 64, 3)\n",
      "******************** Askenasia idx--> 9 ********************\n",
      "(2, 2)\n",
      "2\n",
      "unique--> [10]\n",
      "resized_shape--> (2, 101, 64, 3)\n",
      "******************** Asterompalus_flabellatus idx--> 10 ********************\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "class_dict: defaultdict = defaultdict(dict)\n",
    "dataframes: List = []\n",
    "all_images: List = []\n",
    "desired_image_size = (64,101)\n",
    "\n",
    "for idx, directory in enumerate(glob.glob(\"./common_processed_data/*\")):\n",
    "    resized_images=[]\n",
    "    class_dict[idx] = os.path.basename(directory)\n",
    "    df = pd.read_csv(glob.glob(path.join(directory,\"*.csv\"))[0])\n",
    "    df[\"_target\"]=idx\n",
    "    print(df.shape)\n",
    "    # open image folder and resize all images to same size that corrospond to each row in df above.\n",
    "    images = np.load(glob.glob(path.join(directory,\"*.npy\"))[0],allow_pickle=True)\n",
    "    for im in images:\n",
    "        (h, w) = im.shape[:2]\n",
    "        if w > h:\n",
    "            # calculate the center of the image\n",
    "            im = cv2.rotate(im, cv2.ROTATE_90_CLOCKWISE)\n",
    "            im = cv2.resize(im, desired_image_size)\n",
    "        else:\n",
    "            im = cv2.resize(im, desired_image_size)\n",
    "        resized_images.append(im)\n",
    "        \n",
    "    print(len(resized_images))\n",
    "    # there is one dataframe for each species\n",
    "    dataframes.append(df)\n",
    "    # one set of images for each species\n",
    "    len_res = len(resized_images)\n",
    "    resized_images=np.array(resized_images)\n",
    "    all_images.append(resized_images)\n",
    "    if df.shape[0]==len_res:\n",
    "        print('unique-->',np.unique(df[\"_target\"]))\n",
    "        print('resized_shape-->',resized_images.shape)\n",
    "        print('*'*20,os.path.basename(directory),'idx-->',idx,'*'*20)\n",
    "    else:\n",
    "        print('failed',idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "892e6eae-8c5b-4c18-8031-e0d525d0781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all species dataframes into one \"master\" dataframe\n",
    "path_to_merged = \"./common_merged_data\"\n",
    "os.makedirs(path_to_merged,exist_ok=True)\n",
    "df: pd.DataFrame = pd.concat(dataframes, sort=True)\n",
    "# store information for the entire dataset\n",
    "df.to_csv(path.join(path_to_merged, 'merged_df.csv'))\n",
    "# free up memory\n",
    "del dataframes\n",
    "# similarly stack all images into one numpy matrix\n",
    "all_images: np.array = np.vstack(all_images)\n",
    "np.save(path.join(path_to_merged, 'merged_images.npy'),all_images)\n",
    "np.save(path.join(path_to_merged, \"class_dict.npy\"),class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b6f207d-97ff-494d-8471-53b4e77a063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./common_merged_data/merged_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f391f9f-373c-4591-ada7-e11d1d207031",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images=np.load(\"./common_merged_data/merged_images.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6070741-cf5e-4fff-8b33-a1b378fd2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict=np.load(\"./common_merged_data/class_dict.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c551416-12a3-46d5-bae5-d11aa6e526eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((324, 3), (324, 101, 64, 3))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape,all_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "929e2b44-ea56-4513-9738-02c232860801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict.tolist().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae1baaef-bdda-405d-8596-6fbb7e5dec9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df._target.unique().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b02d6ab-bdad-4b5f-8283-bb27f28a04c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.1    0\n",
       "Unnamed: 0      0\n",
       "_target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8666aaa-4186-46ce-8ff1-93a91c664c9b",
   "metadata": {},
   "source": [
    "### Process and store the stage-2 dataset to stage-3 final split (training, validation and test) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ce48c3b-b321-444f-b71d-f2120d78d4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total columns after modifying df -- 3\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing import (drop_columns, prepare_training_data, process_attributes)\n",
    "\n",
    "# STAGE TWO - DATA PREPROCESSING\n",
    "# This includes feature engineering, standardization and data splitting\n",
    "# deal with missing values / feature engineering\n",
    "# df: pd.DataFrame = df.drop(['Unnamed: 0.1','Unnamed: 0'],axis=1)\n",
    "# drop features that are no longer needed for training\n",
    "\n",
    "print(f'total columns after modifying df -- {len(df.columns)}')\n",
    "\n",
    "min_samples=10\n",
    "\n",
    "''' a custom standard scaler is called from utils inside prepare training data and \n",
    "    rescaler is stored at path below using joblib.dumb'''\n",
    "'''dump(rescaler, './processed_data/std_scaler.bin', compress=True)'''\n",
    "\n",
    "os.makedirs('./common_split_data',exist_ok=True)\n",
    "path_to_store_std_scaler = './common_split_data/std_scaler.bin'\n",
    "\n",
    "trainAttrX, valAttrX, testAttrX, trainImagesX, \\\n",
    "    valImagesX, testImagesX, y_train, y_val, y_test = \\\n",
    "    prepare_training_data(df, all_images, min_samples, path_to_store_std_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a77ef471-8ea5-46ff-8f62-1ed4289756ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((227, 2),\n",
       " (45, 2),\n",
       " (44, 2),\n",
       " (227, 101, 64, 3),\n",
       " (45, 101, 64, 3),\n",
       " (44, 101, 64, 3),\n",
       " (227, 11),\n",
       " (45, 11),\n",
       " (44, 11))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainAttrX.shape, valAttrX.shape, testAttrX.shape, trainImagesX.shape, \\\n",
    "    valImagesX.shape, testImagesX.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df0f2b33-1cf4-499c-80c8-084489dfc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the class dictionary, for later use,\n",
    "# each int key maps to string species value\n",
    "np.save(path.join('./common_split_data/', 'class_dict.npy'),\n",
    "        class_dict,allow_pickle=True)\n",
    "\n",
    "processed_training_data_path=path.join('./common_split_data', \"plankton_data_101x64_final.pkl\")\n",
    "# finally save the training data for future use\n",
    "with open(processed_training_data_path, \"wb\") as f:\n",
    "    pickle.dump((trainAttrX, valAttrX, testAttrX, trainImagesX,\n",
    "                 valImagesX, testImagesX, y_train, y_val, y_test),\n",
    "                f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e094806-3fbe-464e-b491-284b9d45d10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb23034-ebd6-4e1b-a9a4-5d618116f4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flowcam  Env",
   "language": "python",
   "name": "flwcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
