{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0744da4-1946-4d12-aa29-5680d457cb89",
   "metadata": {},
   "source": [
    "# Step-5 Notebook for training ResNet and Combined/Fusion model with and without Image Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40019242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# common library imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from os import listdir, path\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Dropout, Dense, Activation, add,concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.initializers import glorot_uniform, Constant\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, ReduceLROnPlateau,\n",
    "                                        EarlyStopping, TensorBoard, Callback)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "kernel_init = glorot_uniform()\n",
    "bias_init = Constant(value=0.1)\n",
    "regularizer = regularizers.l2(0.0003)\n",
    "IMAGE_SIZE = (101, 64, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9954d72-8dda-4add-b931-52f9000ea1d1",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c571f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cnn_residual_inspired(num_classes, \n",
    "                          input_shape, \n",
    "                          dropout_rate, \n",
    "                          num_modules):\n",
    "    \n",
    "    visible = Input(shape=input_shape)\n",
    "    layer = conv2d_bn_layer(visible, 64, (7, 7))\n",
    "    layer = MaxPooling2D((2, 2), strides=(2, 2))(layer)\n",
    "    layer = residual_module(layer, 64)\n",
    "\n",
    "    if num_modules >= 2:\n",
    "        layer = residual_module(layer, 128)\n",
    "\n",
    "    if num_modules >= 3:\n",
    "        layer = residual_module(layer, 256)\n",
    "\n",
    "    if num_modules >= 4:\n",
    "        layer = residual_module(layer, 512)\n",
    "\n",
    "    if num_modules >= 5:\n",
    "        layer = residual_module(layer, 1024)\n",
    "\n",
    "    if num_modules >= 6:\n",
    "        layer = residual_module(layer, 2048)\n",
    "\n",
    "    # output layers\n",
    "    layer = GlobalAveragePooling2D(name='avg_pool')(layer)\n",
    "    layer = Dropout(dropout_rate)(layer)\n",
    "    layer = Dense(num_classes, activation=\"softmax\")(layer)\n",
    "\n",
    "    model = Model(inputs=visible, outputs=layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5945703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv2d_bn_layer(layer_in: Model, \n",
    "                    num_filters: int, \n",
    "                    filter_size: int) -> Model:\n",
    "    \"\"\"A standard Convolution -> Activation -> Batch Normalisation layer \"\"\"\n",
    "    layer_out = Conv2D(num_filters, (filter_size),\n",
    "                       padding='same',\n",
    "                       activation='relu',\n",
    "                       kernel_initializer=kernel_init,\n",
    "                       bias_initializer=bias_init,\n",
    "                       kernel_regularizer=regularizer)(layer_in)\n",
    "\n",
    "    layer_out = BatchNormalization(axis=3)(layer_out)\n",
    "\n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275c17d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def residual_module(layer_in: Model, \n",
    "                    n_filters: int) -> Model:\n",
    "    \"\"\" Creates a residual module\n",
    "    He, K., X. Zhang, S. Ren, and J. Sun. 2016. \n",
    "    Deep residual learning for image recognition. \n",
    "    Proceedings of the IEEE Computer Society Conference on \n",
    "    Computer Vision and Pattern Recognition. 770â€“778.\n",
    "    \"\"\" \n",
    "    merge_input = layer_in\n",
    "\n",
    "    # check if the number of filters needs to increase\n",
    "    if layer_in.shape[-1] != n_filters:\n",
    "        merge_input = Conv2D(n_filters, (1, 1), padding='same',\n",
    "                             activation='relu', kernel_initializer=kernel_init,\n",
    "                             bias_initializer=bias_init)(layer_in)\n",
    "\n",
    "    # conv1\n",
    "    conv1 = conv2d_bn_layer(layer_in, n_filters, (3, 3))\n",
    "    # conv2\n",
    "    conv2 = conv2d_bn_layer(conv1, n_filters, (3, 3))\n",
    "    # add filters, assumes filters/channels last\n",
    "    layer_out = add([conv2, merge_input])\n",
    "    # activation function\n",
    "    layer_out = Activation('relu')(layer_out)\n",
    "\n",
    "    return layer_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb1c2b8-63a6-4c4d-bb66-52e2b804995a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_augmentation_opt(batch_size: int) -> Dict:\n",
    "    \"\"\" Gets passed to the image augmentation function, configures the\n",
    "    ranges of various augmentations.\"\"\"\n",
    "    return {\n",
    "        'ROT_RANGE': 37,\n",
    "        'WIDTH_SHIFT_RANGE': 0.3,\n",
    "        'HEIGHT_SHIFT_RANGE': 0.3,\n",
    "        'SHEAR_RANGE': 10,\n",
    "        'ZOOM_RANGE': 0.4,\n",
    "        'HOR_FLIP': True,\n",
    "        'VER_FLIP': True,\n",
    "        'BATCH_SIZE': batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50471435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Define a custom callback to clear the session\n",
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "class MemoryUsageLogger(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        process = psutil.Process(os.getpid())\n",
    "        print(f\"Memory usage at epoch {epoch}: {process.memory_info().rss / (1024 ** 2)} MB\")\n",
    "        \n",
    "\n",
    "class KerasModel():\n",
    "    \"\"\" A wrapper around the Keras functioanlity to compile and train\n",
    "    TensorFlow deep learning models \"\"\"\n",
    "\n",
    "    def __init__(self, model, verbose=True):\n",
    "        self.model = model\n",
    "        if verbose:\n",
    "            model.summary()\n",
    "\n",
    "    def compile_model(self, compile_opt: Dict = {}) -> None:\n",
    "        # if no arguments passed, some defaults are provided.\n",
    "        OPT = compile_opt.get('OPT', 'adam')\n",
    "        loss = compile_opt.get('loss', 'categorical_crossentropy')\n",
    "        metrics = compile_opt.get('metrics', ['accuracy'])\n",
    "\n",
    "        self.model.compile(optimizer=OPT, loss=loss,  metrics=metrics)\n",
    "\n",
    "    def evaluate_model(self,\n",
    "                       X_test: np.array,\n",
    "                       y_test: np.array,\n",
    "                       # model: Model,\n",
    "                       eval_opt: Dict = {}) -> float:\n",
    "        \n",
    "        BATCH_SIZE = eval_opt.get('BATCH_SIZE', 128)\n",
    "        VERBOSE = eval_opt.get('VERBOSE', 1)\n",
    "        score = self.model.evaluate(\n",
    "            X_test, y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "        return score\n",
    "\n",
    "    def load_model(self, model_filepath: path) -> None:\n",
    "        self.model = load_model(model_filepath)\n",
    "\n",
    "    def save_model(self, model_filepath: path) -> None:\n",
    "        self.model.save(model_filepath)\n",
    "\n",
    "    def train_model(self,\n",
    "                    training_generator: Tuple[np.array, np.array],\n",
    "                    validation_data: np.array,\n",
    "                    model_name: str,\n",
    "                    y_train: np.array,\n",
    "                    save_model: bool = True,\n",
    "                    training_opt: Dict = {}) -> List:\n",
    "        \n",
    "        # sets default values if training options were not passed\n",
    "        BATCH_SIZE = training_opt.get('BATCH_SIZE', 16)\n",
    "        NB_EPOCH = training_opt.get('NB_EPOCH', 10)\n",
    "        VERBOSE = training_opt.get('VERBOSE', 1)\n",
    "\n",
    "        output_path = path.join('./trained_models', '{}_model.h5'.format(model_name))\n",
    "        \n",
    "        y = [np.where(r == 1)[0][0] for r in y_train]\n",
    "        for v in np.where(~y_train.any(axis=0))[0]:\n",
    "            for _ in range(1000):\n",
    "                y.append(v)\n",
    "        class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                          classes=np.unique(y),\n",
    "                                                           y=y)\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        # setup model training callbacks\n",
    "        # save the best model so far when training\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(\n",
    "            output_path, monitor='val_accuracy', verbose=1,\n",
    "            save_best_only=True, mode='max')\n",
    "\n",
    "        # lower learning rate when models learning has plateaued\n",
    "        lr_drop = ReduceLROnPlateau(\n",
    "            monitor='loss', factor=0.5, patience=8, min_lr=0.000001)\n",
    "\n",
    "        # stop training if signs of model convergence\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=13)\n",
    "\n",
    "        # enables tensorboard from console for diagnostic tools\n",
    "        # tensor_board = TensorBoard(log_dir='Graph', histogram_freq=0,\n",
    "        #                            write_graph=True, write_images=True)\n",
    "\n",
    "        callbacks_list = [checkpoint, lr_drop, early_stopping,ClearMemory(), MemoryUsageLogger()]#, tensor_board]\n",
    "\n",
    "        history = self.model.fit(\n",
    "            training_generator,\n",
    "            steps_per_epoch=y_train.shape[0] // BATCH_SIZE,\n",
    "            validation_data=validation_data,\n",
    "            validation_steps=32,\n",
    "            epochs=NB_EPOCH,\n",
    "            verbose=VERBOSE,\n",
    "            callbacks=callbacks_list,\n",
    "            class_weight=class_weights)\n",
    "\n",
    "        return history\n",
    "\n",
    "    def train_model_with_no_augmentation(self,\n",
    "                                         X_train: np.array,\n",
    "                                         y_train: np.array,\n",
    "                                         X_val: np.array,\n",
    "                                         y_val: np.array,\n",
    "                                         model_name: str,\n",
    "                                         training_opt: Dict = {}) -> List:\n",
    "        \"\"\"Same as above but with no image augmentation, not usually used\"\"\"\n",
    "        BATCH_SIZE = training_opt.get('BATCH_SIZE', 46)\n",
    "        NB_EPOCH = training_opt.get('NB_EPOCH', 250)\n",
    "        VERBOSE = training_opt.get('VERBOSE', 1)\n",
    "\n",
    "        output_path = path.join(\n",
    "            './trained_models', '{}_model.h5'.format(model_name))\n",
    "\n",
    "        y = [np.where(r == 1)[0][0] for r in y_train]\n",
    "        for v in np.where(~y_train.any(axis=0))[0]:\n",
    "            for _ in range(1000):\n",
    "                y.append(v)\n",
    "        classes=np.unique(y)\n",
    "        class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                          classes=classes,\n",
    "                                                          y=y)\n",
    "        class_weights = dict(zip(classes, class_weights))\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(\n",
    "            output_path, monitor='val_accuracy', verbose=1,\n",
    "            save_best_only=True, mode='max')\n",
    "\n",
    "        # lower learning rate when models learning has plateaued\n",
    "        lr_drop = ReduceLROnPlateau(\n",
    "            monitor='loss', factor=0.5, patience=8, min_lr=0.000001)\n",
    "\n",
    "        # stop training if signs of model convergence\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=13)\n",
    "\n",
    "        # enables tensorboard from console for diagnostic tools\n",
    "        # tensor_board = TensorBoard(log_dir='Graph', histogram_freq=0,\n",
    "        #                            write_graph=True, write_images=True)\n",
    "\n",
    "        callbacks_list = [checkpoint, lr_drop, early_stopping,ClearMemory(), MemoryUsageLogger()]#, tensor_board]\n",
    "\n",
    "        history = self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=NB_EPOCH,\n",
    "            verbose=VERBOSE,\n",
    "            callbacks=callbacks_list,\n",
    "            class_weight=class_weights)\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def train_combined_model(self,\n",
    "                            training_generator,\n",
    "                            validation_data,\n",
    "                            model_name: str,\n",
    "                            y_train: np.array,\n",
    "                            training_opt: Dict = {}) -> List:\n",
    "      \n",
    "        BATCH_SIZE = training_opt.get('BATCH_SIZE', 46)\n",
    "        NB_EPOCH = training_opt.get('NB_EPOCH', 250)\n",
    "        VERBOSE = training_opt.get('VERBOSE', 1)\n",
    "\n",
    "        output_path = path.join(\n",
    "            './trained_models', '{}_com_model.h5'.format(model_name))\n",
    "\n",
    "        y = [np.where(r == 1)[0][0] for r in y_train]\n",
    "        for v in np.where(~y_train.any(axis=0))[0]:\n",
    "            for _ in range(1000):\n",
    "                y.append(v)\n",
    "        classes=np.unique(y)\n",
    "        class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                          classes=classes,\n",
    "                                                          y=y)\n",
    "        class_weights = dict(zip(classes, class_weights))\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(\n",
    "            output_path, monitor='val_accuracy', verbose=1,\n",
    "            save_best_only=True, mode='max')\n",
    "        \n",
    "        lr_drop = ReduceLROnPlateau(\n",
    "            monitor='loss', factor=0.5, patience=8, min_lr=0.000001)\n",
    "        \n",
    "        # stop training if signs of model convergence\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=13)\n",
    "\n",
    "        # enables tensorboard from console for diagnostic tools\n",
    "        # tensor_board = TensorBoard(log_dir='Graph', histogram_freq=0,\n",
    "        #                            write_graph=True, write_images=True)\n",
    "\n",
    "        callbacks_list = [checkpoint, lr_drop, early_stopping,ClearMemory(), MemoryUsageLogger()]#, tensor_board]\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            training_generator,\n",
    "            steps_per_epoch=y_train.shape[0] // BATCH_SIZE,\n",
    "            validation_data=validation_data,\n",
    "            validation_steps=32,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=NB_EPOCH,\n",
    "            verbose=VERBOSE,\n",
    "            callbacks=callbacks_list,\n",
    "            class_weight=class_weights)\n",
    "        \n",
    "        return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547aa448-3b0b-4aad-89c1-1094f1b96c27",
   "metadata": {},
   "source": [
    "### Load the training, validation and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e64fe181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "processed_training_data_path = \"./flowcam_split_data/plankton_data_101x64_final.pkl\"\n",
    "with open(processed_training_data_path, \"rb\") as file:\n",
    "    trainAttrX, valAttrX, testAttrX, trainImagesX, \\\n",
    "        valImagesX, testImagesX, y_train, y_val, y_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0b89dd-68cb-4881-a5c4-deeb38053aed",
   "metadata": {},
   "source": [
    "### Calling the defined resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5e98aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropout_rate=0.2\n",
    "num_modules=3\n",
    "model = cnn_residual_inspired(\n",
    "            num_classes=y_train.shape[1],\n",
    "            input_shape=IMAGE_SIZE,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_modules=num_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588fc5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 101, 64, 3)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 101, 64, 64)  9472        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 101, 64, 64)  256        ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 50, 32, 64)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 50, 32, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 50, 32, 64)  256         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 50, 32, 64)   36928       ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 50, 32, 64)  256         ['conv2d_11[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 50, 32, 64)   0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  'max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 50, 32, 64)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 50, 32, 128)  73856       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 50, 32, 128)  512        ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 50, 32, 128)  147584      ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 50, 32, 128)  512        ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 50, 32, 128)  8320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 50, 32, 128)  0           ['batch_normalization_11[0][0]', \n",
      "                                                                  'conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 50, 32, 128)  0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 50, 32, 256)  295168      ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 50, 32, 256)  1024       ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 50, 32, 256)  590080      ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 50, 32, 256)  1024       ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 50, 32, 256)  33024       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 50, 32, 256)  0           ['batch_normalization_13[0][0]', \n",
      "                                                                  'conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 50, 32, 256)  0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 256)         0           ['activation_5[0][0]']           \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 256)          0           ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4)            1028        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,236,228\n",
      "Trainable params: 1,234,308\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model= KerasModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00946e25-df73-48d0-9293-017b43476641",
   "metadata": {},
   "source": [
    "### Defining the image augmentation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d89b2991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_gen_opt: Dict = get_image_augmentation_opt(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5118b583",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ROT_RANGE': 37,\n",
       " 'WIDTH_SHIFT_RANGE': 0.3,\n",
       " 'HEIGHT_SHIFT_RANGE': 0.3,\n",
       " 'SHEAR_RANGE': 10,\n",
       " 'ZOOM_RANGE': 0.4,\n",
       " 'HOR_FLIP': True,\n",
       " 'VER_FLIP': True,\n",
       " 'BATCH_SIZE': 32}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42069846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import create_augmented_images_generator\n",
    "# create_augmented_images_generator(X_attributes: np.array,\n",
    "#                                   X_images: np.array,\n",
    "#                                   Y: np.array,\n",
    "#                                   opt: Dict = {},\n",
    "#                                   only_images: bool = False,\n",
    "#                                   multiple_inputs: bool = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45ba29f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_gen = create_augmented_images_generator(trainAttrX,\n",
    "                                              trainImagesX,\n",
    "                                              y_train,\n",
    "                                              train_gen_opt,\n",
    "                                              only_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af7349-77a0-451c-9485-9fd6b4210a08",
   "metadata": {},
   "source": [
    "### Trainining the ResNet model with image augmentation and storing the weights at \"./trained_models\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18b6e04b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "num_epochs=2\n",
    "cnn_model_name=\"cnn3_with_aug\"\n",
    "\n",
    "training_opt: Dict = {'BATCH_SIZE': batch_size, 'NB_EPOCH': num_epochs}\n",
    "keras_model.compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b17c6d90-8182-41d6-85ac-7249ce37f85d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 16:47:36.373370: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - ETA: 0s - loss: 1.7127 - accuracy: 0.4896 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26087, saving model to ./trained_models/cnn3_with_aug_model.h5\n",
      "Memory usage at epoch 0: 4326.42578125 MB\n",
      "3/3 [==============================] - 6s 560ms/step - loss: 1.7127 - accuracy: 0.4896 - val_loss: 8.6538 - val_accuracy: 0.2609 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9585 - accuracy: 0.7674WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "Memory usage at epoch 1: 4337.48828125 MB\n",
      "3/3 [==============================] - 2s 52ms/step - loss: 0.9585 - accuracy: 0.7674 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = keras_model.train_model(\n",
    "    train_gen,\n",
    "    (valImagesX, y_val),\n",
    "    cnn_model_name,\n",
    "    y_train,\n",
    "    training_opt=training_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149b07d-05b6-4b87-bc73-ddca4655dc8a",
   "metadata": {},
   "source": [
    "### Trainining the ResNet model without image augmentation and storing the weights at \"./trained_models\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3516f259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 101, 64, 3)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 101, 64, 64)  9472        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 101, 64, 64)  256        ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 50, 32, 64)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 50, 32, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 50, 32, 64)  256         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 50, 32, 64)   36928       ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 50, 32, 64)  256         ['conv2d_11[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 50, 32, 64)   0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  'max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 50, 32, 64)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 50, 32, 128)  73856       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 50, 32, 128)  512        ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 50, 32, 128)  147584      ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 50, 32, 128)  512        ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 50, 32, 128)  8320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 50, 32, 128)  0           ['batch_normalization_11[0][0]', \n",
      "                                                                  'conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 50, 32, 128)  0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 50, 32, 256)  295168      ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 50, 32, 256)  1024       ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 50, 32, 256)  590080      ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 50, 32, 256)  1024       ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 50, 32, 256)  33024       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 50, 32, 256)  0           ['batch_normalization_13[0][0]', \n",
      "                                                                  'conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 50, 32, 256)  0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 256)         0           ['activation_5[0][0]']           \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 256)          0           ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4)            1028        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,236,228\n",
      "Trainable params: 1,234,308\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "num_epochs=2\n",
    "cnn_model_name=\"cnn3_no_aug\"\n",
    "\n",
    "training_opt: Dict = {'BATCH_SIZE': batch_size, 'NB_EPOCH': num_epochs}\n",
    "keras_model = KerasModel(model)\n",
    "keras_model.compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a67d6436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5907 - accuracy: 0.8854\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26087, saving model to ./trained_models/cnn3_no_aug_model.h5\n",
      "Memory usage at epoch 0: 4273.578125 MB\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 0.9656 - accuracy: 0.8475 - val_loss: 15.1433 - val_accuracy: 0.2609 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5635 - accuracy: 0.8854WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0146s vs `on_train_batch_end` time: 0.0190s). Check your callbacks.\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.26087\n",
      "Memory usage at epoch 1: 4275.3046875 MB\n",
      "4/4 [==============================] - 1s 75ms/step - loss: 0.5696 - accuracy: 0.8898 - val_loss: 15.2243 - val_accuracy: 0.2609 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = keras_model.train_model_with_no_augmentation(\n",
    "                                         trainImagesX,\n",
    "                                         y_train,\n",
    "                                         valImagesX,\n",
    "                                         y_val,\n",
    "                                         model_name = cnn_model_name,\n",
    "                                         training_opt = training_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94b1c5-a84f-429b-a8bd-5272ecffe036",
   "metadata": {},
   "source": [
    "# Combined or Fusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9df3b4f-ad41-437f-8d82-7fb0c4c29054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_multiple_cnn_models(models: List[Model],\n",
    "                                y: np.array,\n",
    "                                dropout_rate: float = 0.5) -> Model:\n",
    "    \"\"\"This will combine multiple deep learning models\n",
    "    (each individual should have their SoftMax layer removed)\n",
    "    into a single model, by providing a concatanation layer and a fully\n",
    "    connected layer\n",
    "    \"\"\"\n",
    "    combinedInput = concatenate(\n",
    "        [model.output for model in models], name=\"cnn_concat\")\n",
    "\n",
    "    x = Dense(1024,\n",
    "              activation='relu',\n",
    "              kernel_initializer=kernel_init,\n",
    "              bias_initializer=bias_init)(combinedInput)\n",
    "    x = BatchNormalization(name='combined_bn')(x)\n",
    "\n",
    "    x = Dense(y.shape[1], activation='softmax', name='combined_pred')(x)\n",
    "\n",
    "    model = Model(inputs=[model.input for model in models], outputs=x)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07474f73-9d59-4082-9f00-b53839b9c56c",
   "metadata": {},
   "source": [
    "### loading the saved weights of ResNet model and MLP models trained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58824c81-9b7b-4dd4-b0d1-5c1060c02c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1=load_model('./trained_models/cnn3_with_aug_model.h5')\n",
    "model2=load_model('./trained_models/mlp_model_flowcam_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe2cc62f-c937-462f-acec-f5fea8aaa198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = Model(model1.input, model1.layers[-2].output)\n",
    "for l in model1.layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f59feeee-a49a-4cb2-89f3-ef2b6e9e2978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = Model(model2.input, model2.layers[-2].output)\n",
    "for l in model2.layers:\n",
    "    l.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cfd901e-773f-4ccf-9ee9-4abcc966847e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models=[]\n",
    "for i, mod in enumerate([model1,model2]):\n",
    "        for j, layer in enumerate(mod.layers):\n",
    "            layer._name = \"model_{}_layer_{}\".format(i, j)\n",
    "        models.append(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af35032-3af9-4573-8eed-41bd17a740a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calling the Fusion model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a85676c9-07d0-4d79-a123-0158edd4a59c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_com = combine_multiple_cnn_models(models,\n",
    "                                    y_train,\n",
    "                                    dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "863a492a-fd45-4de5-9f42-01a58956a2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " model_0_layer_0 (InputLayer)   [(None, 101, 64, 3)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " model_0_layer_1 (Conv2D)       (None, 101, 64, 64)  9472        ['model_0_layer_0[0][0]']        \n",
      "                                                                                                  \n",
      " model_0_layer_2 (BatchNormaliz  (None, 101, 64, 64)  256        ['model_0_layer_1[0][0]']        \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " model_0_layer_3 (MaxPooling2D)  (None, 50, 32, 64)  0           ['model_0_layer_2[0][0]']        \n",
      "                                                                                                  \n",
      " model_0_layer_4 (Conv2D)       (None, 50, 32, 64)   36928       ['model_0_layer_3[0][0]']        \n",
      "                                                                                                  \n",
      " model_0_layer_5 (BatchNormaliz  (None, 50, 32, 64)  256         ['model_0_layer_4[0][0]']        \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " model_0_layer_6 (Conv2D)       (None, 50, 32, 64)   36928       ['model_0_layer_5[0][0]']        \n",
      "                                                                                                  \n",
      " model_0_layer_7 (BatchNormaliz  (None, 50, 32, 64)  256         ['model_0_layer_6[0][0]']        \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " model_0_layer_8 (Add)          (None, 50, 32, 64)   0           ['model_0_layer_7[0][0]',        \n",
      "                                                                  'model_0_layer_3[0][0]']        \n",
      "                                                                                                  \n",
      " model_0_layer_9 (Activation)   (None, 50, 32, 64)   0           ['model_0_layer_8[0][0]']        \n",
      "                                                                                                  \n",
      " model_0_layer_10 (Conv2D)      (None, 50, 32, 128)  73856       ['model_0_layer_9[0][0]']        \n",
      "                                                                                                  \n",
      " model_0_layer_11 (BatchNormali  (None, 50, 32, 128)  512        ['model_0_layer_10[0][0]']       \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " model_0_layer_12 (Conv2D)      (None, 50, 32, 128)  147584      ['model_0_layer_11[0][0]']       \n",
      "                                                                                                  \n",
      " model_0_layer_13 (BatchNormali  (None, 50, 32, 128)  512        ['model_0_layer_12[0][0]']       \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " model_0_layer_14 (Conv2D)      (None, 50, 32, 128)  8320        ['model_0_layer_9[0][0]']        \n",
      "                                                                                                  \n",
      " model_0_layer_15 (Add)         (None, 50, 32, 128)  0           ['model_0_layer_13[0][0]',       \n",
      "                                                                  'model_0_layer_14[0][0]']       \n",
      "                                                                                                  \n",
      " model_0_layer_16 (Activation)  (None, 50, 32, 128)  0           ['model_0_layer_15[0][0]']       \n",
      "                                                                                                  \n",
      " model_0_layer_17 (Conv2D)      (None, 50, 32, 256)  295168      ['model_0_layer_16[0][0]']       \n",
      "                                                                                                  \n",
      " model_0_layer_18 (BatchNormali  (None, 50, 32, 256)  1024       ['model_0_layer_17[0][0]']       \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " model_0_layer_19 (Conv2D)      (None, 50, 32, 256)  590080      ['model_0_layer_18[0][0]']       \n",
      "                                                                                                  \n",
      " model_1_layer_0 (InputLayer)   [(None, 43)]         0           []                               \n",
      "                                                                                                  \n",
      " model_0_layer_20 (BatchNormali  (None, 50, 32, 256)  1024       ['model_0_layer_19[0][0]']       \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " model_0_layer_21 (Conv2D)      (None, 50, 32, 256)  33024       ['model_0_layer_16[0][0]']       \n",
      "                                                                                                  \n",
      " model_1_layer_1 (Dense)        (None, 1024)         45056       ['model_1_layer_0[0][0]']        \n",
      "                                                                                                  \n",
      " model_0_layer_22 (Add)         (None, 50, 32, 256)  0           ['model_0_layer_20[0][0]',       \n",
      "                                                                  'model_0_layer_21[0][0]']       \n",
      "                                                                                                  \n",
      " model_1_layer_2 (BatchNormaliz  (None, 1024)        4096        ['model_1_layer_1[0][0]']        \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " model_0_layer_23 (Activation)  (None, 50, 32, 256)  0           ['model_0_layer_22[0][0]']       \n",
      "                                                                                                  \n",
      " model_1_layer_3 (Dense)        (None, 512)          524800      ['model_1_layer_2[0][0]']        \n",
      "                                                                                                  \n",
      " model_0_layer_24 (GlobalAverag  (None, 256)         0           ['model_0_layer_23[0][0]']       \n",
      " ePooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " model_1_layer_4 (BatchNormaliz  (None, 512)         2048        ['model_1_layer_3[0][0]']        \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " model_0_layer_25 (Dropout)     (None, 256)          0           ['model_0_layer_24[0][0]']       \n",
      "                                                                                                  \n",
      " model_1_layer_5 (Dropout)      (None, 512)          0           ['model_1_layer_4[0][0]']        \n",
      "                                                                                                  \n",
      " cnn_concat (Concatenate)       (None, 768)          0           ['model_0_layer_25[0][0]',       \n",
      "                                                                  'model_1_layer_5[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         787456      ['cnn_concat[0][0]']             \n",
      "                                                                                                  \n",
      " combined_bn (BatchNormalizatio  (None, 1024)        4096        ['dense[0][0]']                  \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " combined_pred (Dense)          (None, 4)            4100        ['combined_bn[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,606,852\n",
      "Trainable params: 793,604\n",
      "Non-trainable params: 1,813,248\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model = KerasModel(model_com)\n",
    "batch_size=64\n",
    "num_epochs=2\n",
    "\n",
    "# train_gen_opt: Dict = get_image_augmentation_opt(batch_size)\n",
    "\n",
    "val_gen_opt: Dict = {\n",
    "    'ROT_RANGE': 0,\n",
    "    'WIDTH_SHIFT_RANGE': 0.0,\n",
    "    'HEIGHT_SHIFT_RANGE': 0.0,\n",
    "    'SHEAR_RANGE': 0.0,\n",
    "    'ZOOM_RANGE': 0.0,\n",
    "    'HOR_FLIP': False,\n",
    "    'VER_FLIP': False,\n",
    "    'BATCH_SIZE': batch_size}\n",
    "\n",
    "'''WARNING comment below line for making augmentations'''\n",
    "train_gen_opt=val_gen_opt\n",
    "\n",
    "train_gen = create_augmented_images_generator(\n",
    "    trainAttrX, trainImagesX, y_train, train_gen_opt, only_images=False)\n",
    "val_gen = create_augmented_images_generator(\n",
    "    valAttrX, valImagesX, y_val, val_gen_opt, only_images=False)\n",
    "\n",
    "training_opt: Dict = \\\n",
    "    {'BATCH_SIZE': batch_size, 'NB_EPOCH': num_epochs}\n",
    "keras_model.compile_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef8f4f-f743-4b66-a8df-39a7c7a44273",
   "metadata": {},
   "source": [
    "### Training and storing the weights of Fusion model at \"./trained_models\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf8ed9e5-f935-476c-8ee7-d87b800fd28b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3029 - accuracy: 0.2031\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69565, saving model to ./trained_models/cnn_aug_mlp_com_model.h5\n",
      "Memory usage at epoch 0: 5127.12109375 MB\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.3029 - accuracy: 0.2031 - val_loss: 1.2721 - val_accuracy: 0.6957 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.8889\n",
      "Epoch 2: val_accuracy improved from 0.69565 to 0.86957, saving model to ./trained_models/cnn_aug_mlp_com_model.h5\n",
      "Memory usage at epoch 1: 5205.1484375 MB\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6893 - accuracy: 0.8889 - val_loss: 0.9791 - val_accuracy: 0.8696 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "concat_model_name=\"cnn_aug_mlp\"\n",
    "hist = keras_model.train_combined_model(\n",
    "    train_gen,\n",
    "    val_gen,\n",
    "    concat_model_name,\n",
    "    y_train,\n",
    "    training_opt=training_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d684b3-19e0-4473-96e9-035e31d1db78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flowcam  Env",
   "language": "python",
   "name": "flwcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
